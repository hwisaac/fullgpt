{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. 문서 로드 및 분할 중...\n",
      "2. 벡터 스토어 생성 중...\n",
      "3. LLM 설정 중...\n",
      "4. 메모리 설정 중...\n",
      "5. 체인 구성 중...\n",
      "6. RAG 체인 수행 중...\n",
      "\n",
      "--- 첫 번째 질문: Aaronson 은 유죄인가요? ---\n",
      "답변: Aaronson은 유죄가 아닙니다. 문맥 정보에 따르면, 그는 죄를 지은 것이 아니며, 그에 대한 유죄 판결은 조작된 것입니다.\n",
      "\n",
      "--- 두 번째 질문: 그가 테이블에 어떤 메시지를 썼나요? ---\n",
      "답변: 문맥 정보에서 찾을 수 없습니다.\n",
      "\n",
      "--- 세 번째 질문: Julia 는 누구인가요? ---\n",
      "답변: Julia는 주인공의 사랑하는 사람으로, 그와 함께 자유롭게 있을 때보다 그를 더욱 깊이 사랑하게 된 인물입니다. 그녀는 주인공이 고통받고 있는 상황에서도 여전히 살아있고 그의 도움이 필요하다고 느끼게 됩니다.\n",
      "\n",
      "--- 메모리에 저장된 대화 내용 ---\n",
      "Human: Aaronson 은 유죄인가요?\n",
      "AI: Aaronson은 유죄가 아닙니다. 문맥 정보에 따르면, 그는 죄를 지은 것이 아니며, 그에 대한 유죄 판결은 조작된 것입니다.\n",
      "Human: 그가 테이블에 어떤 메시지를 썼나요?\n",
      "AI: 문맥 정보에서 찾을 수 없습니다.\n",
      "Human: Julia 는 누구인가요?\n",
      "AI: Julia는 주인공의 사랑하는 사람으로, 그와 함께 자유롭게 있을 때보다 그를 더욱 깊이 사랑하게 된 인물입니다. 그녀는 주인공이 고통받고 있는 상황에서도 여전히 살아있고 그의 도움이 필요하다고 느끼게 됩니다.\n"
     ]
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.document_loaders import UnstructuredFileLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain, StuffDocumentsChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "# 1. 문서 로드 및 분할\n",
    "print(\"1. 문서 로드 및 분할 중...\")\n",
    "loader = UnstructuredFileLoader(\"./document.txt\")\n",
    "documents = loader.load()\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "splits = text_splitter.split_documents(documents)\n",
    "\n",
    "# 2. 벡터 스토어 생성\n",
    "print(\"2. 벡터 스토어 생성 중...\")\n",
    "embeddings = OpenAIEmbeddings()\n",
    "vectorstore = FAISS.from_documents(splits, embeddings)\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "# 3. LLM 설정\n",
    "print(\"3. LLM 설정 중...\")\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "# 4. 메모리 설정\n",
    "print(\"4. 메모리 설정 중...\")\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\", input_key=\"question\")\n",
    "\n",
    "# 5. Stuff Documents 체인 구성\n",
    "print(\"5. 체인 구성 중...\")\n",
    "\n",
    "# 프롬프트 템플릿 설정\n",
    "prompt_template = \"\"\"\n",
    "당신은 George Orwell의 1984 소설에 관한 질문에 답변하는 도우미입니다.\n",
    "주어진 문맥 정보를 기반으로만 답변하고, 문맥 정보에서 발견할 수 없는 내용은 \"문맥 정보에서 찾을 수 없습니다\"라고 대답하세요.\n",
    "\n",
    "문맥 정보:\n",
    "{context}\n",
    "\n",
    "이전 대화:\n",
    "{chat_history}\n",
    "\n",
    "질문: {question}\n",
    "답변:\n",
    "\"\"\"\n",
    "\n",
    "document_prompt = PromptTemplate(\n",
    "    input_variables=[\"page_content\"], template=\"{page_content}\"\n",
    ")\n",
    "\n",
    "# LLMChain 생성\n",
    "prompt = PromptTemplate(\n",
    "    template=prompt_template, input_variables=[\"context\", \"question\", \"chat_history\"]\n",
    ")\n",
    "llm_chain = LLMChain(llm=llm, prompt=prompt)\n",
    "\n",
    "# StuffDocumentsChain 생성\n",
    "stuff_chain = StuffDocumentsChain(\n",
    "    llm_chain=llm_chain,\n",
    "    document_variable_name=\"context\",\n",
    "    document_prompt=document_prompt,\n",
    ")\n",
    "\n",
    "# 6. 수동으로 RAG 체인 구현\n",
    "print(\"6. RAG 체인 수행 중...\")\n",
    "\n",
    "\n",
    "# 질문 및 답변 함수 정의\n",
    "def ask_question(question):\n",
    "    # 문맥 정보 검색\n",
    "    docs = retriever.get_relevant_documents(question)\n",
    "\n",
    "    # 메모리에서 채팅 기록 가져오기\n",
    "    chat_history = memory.load_memory_variables({})[\"chat_history\"]\n",
    "\n",
    "    # 체인 실행\n",
    "    response = stuff_chain.run(\n",
    "        input_documents=docs, question=question, chat_history=chat_history\n",
    "    )\n",
    "\n",
    "    # 메모리 업데이트\n",
    "    memory.save_context({\"question\": question}, {\"output\": response})\n",
    "\n",
    "    return response\n",
    "\n",
    "\n",
    "# 첫 번째 질문\n",
    "print(\"\\n--- 첫 번째 질문: Aaronson 은 유죄인가요? ---\")\n",
    "answer1 = ask_question(\"Aaronson 은 유죄인가요?\")\n",
    "print(f\"답변: {answer1}\")\n",
    "\n",
    "# 두 번째 질문\n",
    "print(\"\\n--- 두 번째 질문: 그가 테이블에 어떤 메시지를 썼나요? ---\")\n",
    "answer2 = ask_question(\"그가 테이블에 어떤 메시지를 썼나요?\")\n",
    "print(f\"답변: {answer2}\")\n",
    "\n",
    "# 세 번째 질문\n",
    "print(\"\\n--- 세 번째 질문: Julia 는 누구인가요? ---\")\n",
    "answer3 = ask_question(\"Julia 는 누구인가요?\")\n",
    "print(f\"답변: {answer3}\")\n",
    "\n",
    "# 메모리에 저장된 대화 확인\n",
    "print(\"\\n--- 메모리에 저장된 대화 내용 ---\")\n",
    "chat_history = memory.load_memory_variables({})[\"chat_history\"]\n",
    "print(chat_history)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fullgpt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
