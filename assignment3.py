from langchain.prompts import FewShotPromptTemplate, PromptTemplate
from langchain.chains import LLMChain
from langchain.memory import ConversationBufferMemory
from langchain.chat_models import ChatOpenAI
from langchain.callbacks import StreamingStdOutCallbackHandler


chat = ChatOpenAI(
    model="gpt-4o-mini",
    temperature=0.1,
    streaming=True,
    callbacks=[
        StreamingStdOutCallbackHandler(),
    ],
)

# ì˜ˆì œ ë°ì´í„°
examples = [
    {"input": "Top Gun", "output": "ğŸ›©ï¸ğŸ‘¨â€âœˆï¸ğŸ”¥"},
    {"input": "The Godfather", "output": "ğŸ‘¨â€ğŸ‘¨â€ğŸ‘¦ğŸ”«ğŸ"},
    {"input": "Finding Nemo", "output": "ğŸ ğŸ‘¨â€ğŸ‘¦ğŸŒŠ"},
    {"input": "Titanic", "output": "ğŸš¢â¤ï¸ğŸ’”"},
]

# ì˜ˆì œ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì •ì˜
example_template = PromptTemplate(
    input_variables=["input", "output"], template="Movie: {input}\nEmojis: {output}"
)

# FewShotPromptTemplate ì„¤ì •
few_shot_prompt = FewShotPromptTemplate(
    examples=examples,
    example_prompt=example_template,
    prefix="You are a movie bot that responds with exactly three emojis representing a movie.",
    suffix="Movie: {movie_title}\nEmojis:",
    input_variables=["movie_title"],
)

# ë©”ëª¨ë¦¬ ì„¤ì • - ConversationBufferMemory ì‚¬ìš©
memory = ConversationBufferMemory(input_key="movie_title", memory_key="chat_history")

# LLMChainì„ ì‚¬ìš©í•œ ì˜í™” ì´ëª¨í‹°ì½˜ ë³€í™˜ ì²´ì¸
movie_emoji_chain = LLMChain(
    llm=chat, prompt=few_shot_prompt, memory=memory, verbose=True
)

# ì²´ì¸ í…ŒìŠ¤íŠ¸
if __name__ == "__main__":
    print("\n--- ì²« ë²ˆì§¸ ì˜í™” ---")
    result1 = movie_emoji_chain.run(movie_title="ì¸í„°ìŠ¤í…”ë¼")
    print(f"ì˜í™”: ì¸í„°ìŠ¤í…”ë¼\nì´ëª¨í‹°ì½˜: {result1}")

    print("\n--- ë‘ ë²ˆì§¸ ì˜í™” ---")
    result2 = movie_emoji_chain.run(movie_title="í•´ë¦¬ í¬í„°")
    print(f"ì˜í™”: í•´ë¦¬ í¬í„°\nì´ëª¨í‹°ì½˜: {result2}")

    print("\n--- ë©”ëª¨ë¦¬ í…ŒìŠ¤íŠ¸ ---")
    # ì´ì „ì— ë¬¼ì–´ë³¸ ì˜í™”ë¥¼ í™•ì¸í•˜ê¸° ìœ„í•œ ìƒˆë¡œìš´ ì²´ì¸ ìƒì„±
    # ë©”ëª¨ë¦¬ì˜ ë‚´ìš©ì„ ë¶ˆëŸ¬ì˜¤ê¸°
    chat_history = memory.load_memory_variables({})["chat_history"]

    # ìƒˆë¡œìš´ í”„ë¡¬í”„íŠ¸ì™€ ì²´ì¸ì„ ì‚¬ìš©
    memory_test_prompt = PromptTemplate(
        template="ë‹¤ìŒì€ ì´ì „ ëŒ€í™” ê¸°ë¡ì…ë‹ˆë‹¤:\n{chat_history}\n\nìœ„ ëŒ€í™”ì—ì„œ ì–¸ê¸‰ëœ ì˜í™” ì œëª©ë“¤ì„ ì•Œë ¤ì£¼ì„¸ìš”.",
        input_variables=["chat_history"],
    )

    # ì´ ì²´ì¸ì—ëŠ” ë©”ëª¨ë¦¬ë¥¼ ì—°ê²°í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤
    memory_test_chain = LLMChain(llm=chat, prompt=memory_test_prompt, verbose=True)

    memory_result = memory_test_chain.run(chat_history=chat_history)
    print("ë©”ëª¨ë¦¬ í…ŒìŠ¤íŠ¸ ê²°ê³¼:")
    print(memory_result)
